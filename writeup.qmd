---
title: "Final Project"
author: "Dylan Xia  Kevin Yao Yuqing Wen"
date: "2024/12/01"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---
    - Partner 1 : Dylan Xia (dizhexia), Section 2, github: DizheXia
    - Partner 2 : Kevin Yao (qiyin), Section 3, github: UchicagoKevinYao
    - Partner 3 : Yuqing Wen (wyuqing), Section 2, github: wwyuqing

Research question: 
Our project explores the economic impacts of interest rate changes made by the Federal Reserve on unemployment and spending patterns across the United 
States. 

The central focus was on analyzing how these adjustments, during the low-interest-rate period of 2011–2015 and the relatively high-interest-rate period of 
2016–2019, influenced state-level unemployment trends and urban consumer spending patterns measured by the Consumer Price Index (CPI). We chose to compare these two 
periods because of the Federal Reserve’s decision to increase the interest rate in late 2015. 

In addition, we explored the sentiment and tone of Federal Reserve speeches and announcements to understand how these policies were conveyed to the public. The below 
are the key questions we try to focus on:

-	How does unemployment vary across states during these periods?
-	What are the trends in Consumer Price Index (CPI) in urban areas?
-	How does the tone and specific wording in Federal Reserve communications reflect policy strategies?

Approach/methods: 
To address these questions, we collected data from credible sources, including the Bureau of Labor Statistics for unemployment and CPI data, and official Federal Reserve communications for textual analysis. 
Unemployment rates were aggregated from county to state levels for more accessible regional analysis. CPI data, reflecting urban spending patterns, allowed us to examine inflation trends over time. 
For Fed communication analysis, we also did research on official press release documents for detailed wording comparison and analysis.

Our methodology incorporated various visualization and analysis tools. For unemployment data, we analyzed yearly averages and used geopandas to create 
choropleth maps that visualized regional disparities, and a difference map to highlight significant differences between the two periods. 
For CPI data, we used Altair to create line plots demonstrating temporal trends and scatter plots comparing changes across urban areas between the two periods. 
For textual analysis of Fed communications, we applied NLP techniques and used polarity scores to compare speeches versus announcements to evaluate sentiment and tone. 
In addition, we developed a Shiny app that integrates all our visualizations into an interactive platform, allowing users to explore our findings dynamically.

Weakness/difficulties: 
A key difficulty we encountered was working with unemployment data, which was only available on a county-by-county basis and differentiated by year on the official 
website. These particle data required more preprocessing and recalculation to make them appropriate for our analysis. 

Specifically, we merged county-level data and recalculated it on a state-by-state basis, taking yearly averages and then recalculating mean values for each time 
interval (2011–2015 and 2016–2019). This process required more computation and involved careful handling of missing values and inconsistencies in the dataset.

Another challenge was balancing precision and generalization. Aggregating data at the state level helped simplify our visualizations and analysis but may have ignored more 
localized trends and variations within states. For CPI data, which was only available to download for urban areas, we faced limitations in exploring non-urban trends, as 
the dataset inherently excluded rural regions.

```{python}
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import shape
import requests
import altair as alt
import seaborn as sns

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```

# Step 1: Plot data based on Unemployment Rate
Plot descriptions for unemployment rate: 

In general, it declined significantly between 2011 and 2019, with a sharp increase during the pandemic in 2020. The West and Southeast showed the greatest reductions in 
unemployment, particularly in states like Nevada and California. 

In contrast, the Midwest and East saw relatively little reductions or even no differences. Including 2020 data revealed less obvious improvements in unemployment.

## Unemployment Rate General Trend (2011~2020)
```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the unemployment data
data_path = "state_yearly_unemployment_rate_with_state_name.csv"
unemployment_data = pd.read_csv(data_path)

# Step 2: Calculate the average unemployment rate by year
# Assuming the dataset contains columns: 'year', 'unemployment_rate'
yearly_avg_unemployment = (
    unemployment_data.groupby('year')['unemployment_rate']
    .mean()
    .reset_index()
)

# Convert unemployment rate to percentage
yearly_avg_unemployment['unemployment_rate'] = yearly_avg_unemployment['unemployment_rate'] * 100

# Step 3: Plot the line chart
plt.figure(figsize=(12, 6))

plt.plot(
    yearly_avg_unemployment['year'], 
    yearly_avg_unemployment['unemployment_rate'], 
    marker='o', 
    linestyle='-', 
    label='Average Unemployment Rate'
)

# Add labels and title
plt.xlabel('Year', fontsize=12)
plt.ylabel('Unemployment Rate (%)', fontsize=12)
plt.title('Yearly Average Unemployment Rate (All States)', fontsize=16)

# Customize grid and legend
plt.grid(visible=True, linestyle='--', alpha=0.6)
plt.legend(fontsize=12)
plt.xticks(yearly_avg_unemployment['year'], rotation=45)
plt.tight_layout()

plt.show()

```

## Unemployment Rate (2011~2015) & Unemployment Rate (2016~2020), Map by State

```{python}
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import shape
import requests

# Load the unemployment data
data_path = "state_yearly_unemployment_rate_with_state_name.csv"
unemployment_data = pd.read_csv(data_path)

# Calculate average unemployment rates for 2011-2015
unemployment_avg_2011_2015 = (
    unemployment_data[(unemployment_data['year'] >= 2011) & (unemployment_data['year'] <= 2015)]
    .groupby('state_name')['unemployment_rate']
    .mean()
    .reset_index()
)
unemployment_avg_2011_2015['unemployment_rate'] = unemployment_avg_2011_2015['unemployment_rate'] * 100

# Calculate average unemployment rates for 2016-2019
unemployment_avg_2016_2019 = (
    unemployment_data[(unemployment_data['year'] >= 2016) & (unemployment_data['year'] <= 2019)]
    .groupby('state_name')['unemployment_rate']
    .mean()
    .reset_index()
)
unemployment_avg_2016_2019['unemployment_rate'] = unemployment_avg_2016_2019['unemployment_rate'] * 100

# Find global vmin and vmax for consistent coloring
all_data = pd.concat([unemployment_avg_2011_2015, unemployment_avg_2016_2019])
vmin = all_data['unemployment_rate'].min()
vmax = all_data['unemployment_rate'].max()

# Load GeoJSON data
shape_url = 'https://data.ojp.usdoj.gov/resource/5fdt-n5ne.json'
response = requests.get(shape_url)

if response.status_code == 200:
    geo_data = response.json()
else:
    raise ValueError(f"Failed to fetch GeoJSON data. HTTP Status Code: {response.status_code}")

geometries = [shape(feature["the_geom"]) for feature in geo_data]
properties = [{key: feature[key] for key in feature if key != "the_geom"} for feature in geo_data]
shape_data = gpd.GeoDataFrame(properties, geometry=geometries)

# Function to plot a map with specified data
def plot_map(data, title):
    data['state_name'] = data['state_name'].str.strip()
    merged_data = shape_data.merge(data, left_on='state', right_on='state_name', how='left')
    merged_data['unemployment_rate'] = merged_data['unemployment_rate'].fillna(0)
    
    # Plot map
    fig, ax = plt.subplots(1, 1, figsize=(15, 10))
    merged_data.plot(
        column='unemployment_rate', 
        cmap='Blues', 
        linewidth=0.8, 
        ax=ax, 
        edgecolor='black', 
        legend=True,
        legend_kwds={
            'shrink': 0.7, 
            'orientation': "horizontal", 
            'pad': 0.05, 
            'aspect': 40, 
            'label': "Unemployment Rate (%)"
        },
        vmin=vmin,  
        vmax=vmax  
    )
    merged_data.boundary.plot(ax=ax, linewidth=0.8, color="black")
    ax.set_xlim([-180, -60])  
    ax.set_ylim([15, 72])     
    ax.set_title(title, fontsize=16)
    ax.set_axis_off()
    plt.tight_layout()
    plt.show()

# Plot maps for 2011-2015 and 2016-2019
plot_map(unemployment_avg_2011_2015, 'Average Unemployment Rate by State (2011-2015)')
plot_map(unemployment_avg_2016_2019, 'Average Unemployment Rate by State (2016-2019)')

```

## Unemployment Rate (Difference), Map by State

```{python}

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import shape
import requests

# Load the unemployment data
data_path = "state_yearly_unemployment_rate_with_state_name.csv"
unemployment_data = pd.read_csv(data_path)

# Calculate average unemployment rates for 2011-2015 and 2016-2019 by state
unemployment_avg_2011_2015 = (
    unemployment_data[(unemployment_data['year'] >= 2011) & (unemployment_data['year'] <= 2015)]
    .groupby('state_name')['unemployment_rate']
    .mean()
    .reset_index()
)
unemployment_avg_2016_2019 = (
    unemployment_data[(unemployment_data['year'] >= 2016) & (unemployment_data['year'] <= 2019)]
    .groupby('state_name')['unemployment_rate']
    .mean()
    .reset_index()
)

# Convert to percentage
unemployment_avg_2011_2015['unemployment_rate'] = unemployment_avg_2011_2015['unemployment_rate'] * 100
unemployment_avg_2016_2019['unemployment_rate'] = unemployment_avg_2016_2019['unemployment_rate'] * 100

# Calculate the difference between 2016-2019 and 2011-2015
unemployment_diff = pd.merge(
    unemployment_avg_2016_2019, 
    unemployment_avg_2011_2015, 
    on='state_name', 
    suffixes=('_2016_2019', '_2011_2015')
)
unemployment_diff['rate_difference'] = (
    unemployment_diff['unemployment_rate_2016_2019'] - unemployment_diff['unemployment_rate_2011_2015']
)

# Load the JSON data from the API endpoint for state boundaries
shape_url = 'https://data.ojp.usdoj.gov/resource/5fdt-n5ne.json'
response = requests.get(shape_url)

if response.status_code == 200:
    geo_data = response.json()
else:
    raise ValueError(f"Failed to fetch GeoJSON data. HTTP Status Code: {response.status_code}")

# Convert JSON data to a GeoDataFrame
geometries = [shape(feature["the_geom"]) for feature in geo_data]
properties = [{key: feature[key] for key in feature if key != "the_geom"} for feature in geo_data]
shape_data = gpd.GeoDataFrame(properties, geometry=geometries)

# Merge shape data with unemployment difference data
unemployment_diff['state_name'] = unemployment_diff['state_name'].str.strip()
merged_data = shape_data.merge(unemployment_diff, left_on='state', right_on='state_name', how='left')

# Fill missing values for states with no data
merged_data['rate_difference'] = merged_data['rate_difference'].fillna(0)

# Plot the unemployment difference map
fig, ax = plt.subplots(1, 1, figsize=(15, 10))

# Plot unemployment difference using a diverging colormap
merged_data.plot(
    column='rate_difference', 
    cmap='RdYlGn_r',  
    linewidth=0.8, 
    ax=ax, 
    edgecolor='black', 
    legend=True,
    legend_kwds={
        'shrink': 0.7, 
        'orientation': "horizontal", 
        'pad': 0.05, 
        'aspect': 40, 
        'label': "Unemployment Rate Difference (%)"
    }
)

# Add state boundaries
merged_data.boundary.plot(ax=ax, linewidth=0.8, color="black")

# Adjust map extent to include Alaska and Hawaii
ax.set_xlim([-180, -60])  
ax.set_ylim([15, 72])    

# Customize the plot further for better visualization
ax.set_title('Average Unemployment Rate Difference by State (2016-2019 vs 2011-2015)', fontsize=16)
ax.set_axis_off()
plt.tight_layout()

plt.show()

```

# Step 2: Plot data based on CPI
Plot description for CPI: 
CPI revealed a consistent upward trend, reflecting gradual inflation. Urban areas such as California experienced significant CPI increases marked in red, while regions 
like Ohio and Wisconsin saw minimal changes marked in green.

## CPI Rate General Trend (2011~2020)
```{python}

# Load the CPI data from the Excel file
file_path = 'CPI_urban_2011-2020_2.0.xlsx'
cpi_data = pd.ExcelFile(file_path)

# Parse the relevant sheet
cpi_sheet = cpi_data.parse('BLS Data Series')

# Step 1: Clean the CPI data
# Skip the initial rows to extract relevant data
cpi_cleaned = cpi_sheet.iloc[3:]

# Rename columns for clarity
cpi_cleaned.columns = ['Series ID', 'Area Description'] + [f'Annual_{year}' for year in range(2011, 2021)]

# Keep only relevant columns (Area Description and CPI values)
cpi_cleaned = cpi_cleaned[['Area Description'] + [f'Annual_{year}' for year in range(2011, 2021)]]

# Drop rows with missing Area Description (e.g., aggregated regions or unrelated data)
cpi_cleaned = cpi_cleaned.dropna(subset=['Area Description'])

# Step 2: Calculate the average CPI for all regions for each year
average_cpi_by_year = cpi_cleaned[[f'Annual_{year}' for year in range(2011, 2021)]].mean()

# Prepare data for plotting
years = list(range(2011, 2021))
average_cpi = average_cpi_by_year.values

# Step 3: Plot the line chart
plt.figure(figsize=(10, 6))
plt.plot(years, average_cpi, marker='o', linestyle='-', color='blue', label='Average CPI (All Regions)')

# Add labels, title, and legend
plt.xlabel('Year', fontsize=12)
plt.ylabel('Average CPI', fontsize=12)
plt.title('Average Consumer Price Index (2011–2020)', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.7)
plt.legend()
plt.xticks(years, rotation=45)
plt.tight_layout()

plt.show()

```

## CPI Rate Scatter (Self-Regression)

```{python}
# Load the CPI data
file_path = 'CPI_urban_2011-2020_2.0.xlsx'  # Update this to your local file path
cpi_data = pd.ExcelFile(file_path)
cpi_sheet = cpi_data.parse('BLS Data Series')

# Clean the data
cpi_cleaned = cpi_sheet.iloc[3:]
cpi_cleaned.columns = ['Series ID', 'Area Description'] + [f'Annual_{year}' for year in range(2011, 2021)]
cpi_cleaned = cpi_cleaned[['Area Description'] + [f'Annual_{year}' for year in range(2011, 2021)]]
cpi_cleaned = cpi_cleaned.dropna(subset=['Area Description'])

# Convert annual columns to numeric
for year in range(2011, 2021):
    cpi_cleaned[f'Annual_{year}'] = pd.to_numeric(cpi_cleaned[f'Annual_{year}'], errors='coerce')

# Calculate average CPI for 2011-2015 and 2016-2019
cpi_cleaned['CPI_2011_2015'] = cpi_cleaned[[f'Annual_{year}' for year in range(2011, 2016)]].mean(axis=1)
cpi_cleaned['CPI_2016_2019'] = cpi_cleaned[[f'Annual_{year}' for year in range(2016, 2020)]].mean(axis=1)

# Drop rows where averages could not be computed
cpi_cleaned = cpi_cleaned.dropna(subset=['CPI_2011_2015', 'CPI_2016_2019'])

# Calculate percentage change
cpi_cleaned['CPI_Percent_Change'] = ((cpi_cleaned['CPI_2016_2019'] - cpi_cleaned['CPI_2011_2015']) / cpi_cleaned['CPI_2011_2015']) * 100

# Identify top 5 and bottom 5 areas
top5_changes = cpi_cleaned.nlargest(5, 'CPI_Percent_Change')
bottom5_changes = cpi_cleaned.nsmallest(5, 'CPI_Percent_Change')

# Add jittered labels for visualization
top5_changes['Label'] = top5_changes['Area Description']
top5_changes['Jitter_X'] = top5_changes['CPI_2011_2015'] + 10
top5_changes['Jitter_Y'] = top5_changes['CPI_2016_2019'] + 20

bottom5_changes['Label'] = bottom5_changes['Area Description']
bottom5_changes['Jitter_X'] = bottom5_changes['CPI_2011_2015'] + 10
bottom5_changes['Jitter_Y'] = bottom5_changes['CPI_2016_2019'] - 20

# Create diagonal y=x line
diagonal_data = pd.DataFrame({
    'x': [cpi_cleaned['CPI_2011_2015'].min(), cpi_cleaned['CPI_2011_2015'].max()],
    'y': [cpi_cleaned['CPI_2011_2015'].min(), cpi_cleaned['CPI_2011_2015'].max()]
})

diagonal_line = alt.Chart(diagonal_data).mark_line(strokeDash=[5, 5], color='gray').encode(
    x='x',
    y='y'
)

# Scatter plot for all areas
scatter = alt.Chart(cpi_cleaned).mark_circle(size=60, color='blue').encode(
    x=alt.X('CPI_2011_2015', title='Average CPI (2011–2015)'),
    y=alt.Y('CPI_2016_2019', title='Average CPI (2016–2019)'),
    tooltip=['Area Description', 'CPI_2011_2015', 'CPI_2016_2019', 'CPI_Percent_Change']
)

# Highlight top 5 areas
top5 = alt.Chart(top5_changes).mark_circle(size=100, color='red').encode(
    x='CPI_2011_2015',
    y='CPI_2016_2019',
    tooltip=['Area Description', 'CPI_2011_2015', 'CPI_2016_2019', 'CPI_Percent_Change']
)

top5_labels = alt.Chart(top5_changes).mark_text(align='left', fontSize=12, color='red').encode(
    x='Jitter_X',
    y='Jitter_Y',
    text='Label'
)

# Highlight bottom 5 areas
bottom5 = alt.Chart(bottom5_changes).mark_circle(size=100, color='green').encode(
    x='CPI_2011_2015',
    y='CPI_2016_2019',
    tooltip=['Area Description', 'CPI_2011_2015', 'CPI_2016_2019', 'CPI_Percent_Change']
)

bottom5_labels = alt.Chart(bottom5_changes).mark_text(align='left', fontSize=12, color='green').encode(
    x='Jitter_X',
    y='Jitter_Y',
    text='Label'
)

# Tables for top 5 and bottom 5 areas
top5_table = alt.Chart(top5_changes).mark_text(align='left', fontSize=12, color='red').encode(
    y=alt.Y('row_number:O', axis=None),
    text='Label:N'
).transform_window(
    row_number='row_number()'
).properties(
    title='Top 5 Areas by Percentage Change',
    width=300
)

bottom5_table = alt.Chart(bottom5_changes).mark_text(align='left', fontSize=12, color='green').encode(
    y=alt.Y('row_number:O', axis=None),
    text='Label:N'
).transform_window(
    row_number='row_number()'
).properties(
    title='Bottom 5 Areas by Percentage Change',
    width=300
)

# Combine scatter plot with updated tables
scatter_chart = (scatter + top5 + top5_labels + bottom5 + bottom5_labels + diagonal_line).properties(
    title='Scatter Plot and Highlights of CPI (2011–2015 vs. 2016–2019)',
    width=800,
    height=500
)

final_chart = alt.hconcat(
    scatter_chart,
    alt.vconcat(top5_table, bottom5_table).properties(title='Top and Bottom 5 Areas (Colored)')
)

final_chart

```

# NLP:
For NLP: Sentiment analysis using NLP showed that Fed speeches had a more emotionally engaging tone, as reflected in higher polarity scores, while announcements maintained 
a more neutral tone, prioritizing clarity and consistency.




# Shiny app
Shiny app: To make our findings more interactive, we developed a Shiny app that includes most plots and maps we created. It allows users to explore the 
unemployment rate, CPI trends, and polarity plots through dynamic visualizations. 

In more detail, the app has two dropdown menus: one for time intervals (‘2011-2015’, ‘2016-2020’), and another for visualization types (‘NLP Analysis’ and other ‘Plots’). Once ‘NLP Analysis’ is selected, two polarity plots appear at the same time. 

Besides, after clicking on the ‘Plots’ option for visualization type, you can see three radio buttons (unemployment rate, CPI, and differences). And the unemployment 
rate is special in that you can use the slider to see each year's corresponding map dynamically.
```{python}
from shiny import App, render, ui
import altair as alt
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import os

# Load unemployment data
data_path = "/Users/alina./Desktop/Final-Project_Dylan-Kevin-Yuqing-main/Unemployment Rate Map & CPI Plot/state_yearly_unemployment_rate_with_state_name.csv"
unemployment_data = pd.read_csv(data_path)

# Rename columns for consistency
unemployment_data.rename(columns={'state_name': 'State', 'year': 'Year'}, inplace=True)

# Load GeoJSON data for US states
shape_url = 'https://raw.githubusercontent.com/PublicaMundi/MappingAPI/master/data/geojson/us-states.json'
shape_data = gpd.read_file(shape_url)

# Directory to store generated maps
output_dir = "/tmp/maps"
os.makedirs(output_dir, exist_ok=True)

# Function to dynamically create maps
def create_map(year):
    # Filter data for the selected year
    filtered_data = unemployment_data[unemployment_data['Year'] == year]
    filtered_data = filtered_data.groupby('State', as_index=False)['unemployment_rate'].mean()
    filtered_data['unemployment_rate'] = filtered_data['unemployment_rate'] * 100 

    # Merge with shape data
    merged_data = shape_data.merge(filtered_data, left_on='name', right_on='State', how='left')
    merged_data['unemployment_rate'] = merged_data['unemployment_rate'].fillna(0)  

    # Plot map
    fig, ax = plt.subplots(1, 1, figsize=(15, 10))
    merged_data.plot(
        column='unemployment_rate',
        cmap='Blues',
        linewidth=0.8,
        ax=ax,
        edgecolor='black',
        legend=True,
        legend_kwds={
            'shrink': 0.7,
            'orientation': "horizontal",
            'label': "Unemployment Rate (%)"
        }
    )
    ax.set_title(f"Unemployment Rate by State ({year})", fontsize=16)
    ax.set_axis_off()
    
    # Save map to file
    filepath = os.path.join(output_dir, f"map_{year}.png")
    plt.savefig(filepath, dpi=300)
    plt.close(fig)
    return filepath

# Define UI
app_ui = ui.page_fluid(
    # Dropdown for Time Period Selection
    ui.input_select(
        "time_period",
        "Select a Time Period:",
        choices=["2011-2015", "2016-2020"]
    ),
    # Dropdown for Visualization Selection
    ui.input_select(
        "visualization_type",
        "Select Visualization Type:",
        choices=["Plots", "NLP Analysis"]
    ),
    # Conditional rendering for data type (only for "Plots")
    ui.output_ui("data_type_buttons"),
    
    # Conditional rendering for CPI plots
    ui.panel_conditional(
        "input.visualization_type === 'Plots' && input.data_type === 'CPI'",
        ui.output_image("cpi_plot", width="100%", height="600px"),
    ),
    # Conditional rendering for unemployment maps (dynamic by year)
    ui.panel_conditional(
        "input.visualization_type === 'Plots' && input.data_type === 'Unemployment rate'",
        ui.input_slider("year_slider", "Select Year", min=2011, max=2020, value=2015, step=1),
        ui.output_image("dynamic_unemployment_map", width="100%", height="600px")
    ),
    # Conditional rendering for difference plot
    ui.panel_conditional(
        "input.visualization_type === 'Plots' && input.data_type === 'Differences'",
        ui.output_image("difference_plot", width="100%", height="600px")
    ),
    # Conditional rendering for NLP Analysis
    ui.panel_conditional(
        "input.visualization_type === 'NLP Analysis'",
        ui.output_image("nlp_plot1", width="100%", height="600px"),
        ui.br(),
        ui.output_image("nlp_plot2", width="100%", height="600px")
    ),
    # Outputs for selected dropdown and visualization type
    ui.output_text_verbatim("dropdown_total"),
    ui.output_text_verbatim("visualization_type_output")
)

# Define Server Logic
def server(input, output, session):
    @output
    @render.ui
    def data_type_buttons():
        # Render radio buttons dynamically for "Plots"
        if input.visualization_type() == "Plots":
            return ui.input_radio_buttons(
                "data_type",
                "Select Data Type:",
                choices=["Unemployment rate", "CPI", "Differences"]
            )
        return None

    @output
    @render.image
    def dynamic_unemployment_map():
        if input.visualization_type() == "Plots" and input.data_type() == "Unemployment rate":
            year = input.year_slider()
            map_path = create_map(year)
            return {"src": map_path, "alt": f"Unemployment Rate Map for {year}"}
        return None

    @output
    @render.image
    def difference_plot():
        # Render difference plot
        if input.visualization_type() == "Plots" and input.data_type() == "Differences":
            return {
                "src": "/Users/alina./Desktop/Final-Project_Dylan-Kevin-Yuqing-main/pictures/Difference_graph_2011-2019.png",
                "alt": "Differences in Unemployment Rate"
            }
        return None

    @output
    @render.image
    def cpi_plot():
        # Render CPI Analysis plot
        if input.visualization_type() == "Plots" and input.data_type() == "CPI":
            return {
                "src": "/Users/alina./Desktop/Final-Project_Dylan-Kevin-Yuqing-main/pictures/CPI_analysis.png",
                "alt": "CPI Analysis"
            }
        return None

    @output
    @render.image
    def nlp_plot1():
        # Render first NLP plot
        if input.visualization_type() == "NLP Analysis":
            return {
                "src": "/Users/alina./Desktop/Final-Project_Dylan-Kevin-Yuqing-main/pictures/Poliarity_Announcement.png",
                "alt": "FOMC Announcement Polarity"
            }
        return None

    @output
    @render.image
    def nlp_plot2():
        # Render second NLP plot
        if input.visualization_type() == "NLP Analysis":
            return {
                "src": "/Users/alina./Desktop/Final-Project_Dylan-Kevin-Yuqing-main/pictures/Polarity_Speech.png",
                "alt": "Fed Chair Speech Polarity"
            }
        return None

    @output
    @render.text
    def dropdown_total():
        return f"You selected: {input.time_period()}"

    @output
    @render.text
    def visualization_type_output():
        return f"Visualization Type: {input.visualization_type()}"

app = App(app_ui, server)
```

Policy implications: 
Our findings reveal some key policy implications for addressing the various impacts of interest rate changes. Regional differences in unemployment trends highlight the 
need for targeted interventions, such as targeted job creation programs in regions like the Midwest, which showed little improvement over time. 

The CPI analysis underscores pressures of inflation in urban areas, suggesting policymakers consider tax adjustments or subsidies to support urban households while 
promoting rural economic development. The balance between controlling the inflation and supporting employment also shows the importance of measuring the rate changes. 

Lastly, the NLP analysis suggests that appropriate and flexible communication strategies can increase public trust and reduce uncertainty, emphasizing the 
importance of effective messaging.


Future work: 
In conclusion, our project shows the diverse regional impacts of interest rate changes. While unemployment decreased overall, the degree of recovery varied 
significantly across states, and the inflation was more obvious in urban areas like California. 

Sentiment/tone analysis demonstrated the Federal Reserve’s dual strategy 
of engaging the public emotionally through speeches while maintaining precision in official announcements. 

Future research could expand in various directions. For example, including more indicators such as wage growth and housing prices would provide a more comprehensive 
understanding of economic ripple effects. While we analyze urban data, exploring rural CPI trends can address regional differences and provide a broader perspective. 

Extending the timeframe to include data before 2011 or after 2019 would also capture longer-term trends and the effects of recent economic events, such as the pandemic. 
These directions would strengthen the foundation we’ve already explored and open up new ways to understand the impact of interest rate changes.